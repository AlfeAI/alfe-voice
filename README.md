# Alfe Voice

> Voice-agnostic agent communication layer â€” connect AI agents to any voice channel.

[![TypeScript](https://img.shields.io/badge/TypeScript-5.x-blue)](https://www.typescriptlang.org/)
[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE)

**Alfe Voice** is the voice layer for [Alfe AI](https://alfe.ai). It connects OpenClaw-powered AI agents to any voice channel â€” phone calls, Google Meet, Slack voice memos, or browser-based WebRTC â€” through a unified STT â†’ Agent â†’ TTS pipeline.

ğŸŒ **[voice.alfe.ai](https://voice.alfe.ai)**

## Why Alfe Voice?

AI agents shouldn't be locked to text. Alfe Voice makes any agent voice-capable across every channel:

- **ğŸ“ Phone** â€” Twilio-powered inbound/outbound calls
- **ğŸ¥ Google Meet** â€” Join meetings as a participant via WebRTC
- **ğŸ’¬ Slack** â€” Transcribe and respond to voice memos
- **ğŸŒ Browser** â€” Real-time voice in the browser via WebRTC

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Channel    â”‚â”€â”€â”€â”€â–¶â”‚  Alfe Voice   â”‚â”€â”€â”€â”€â–¶â”‚  AI Agent   â”‚
â”‚  (Twilio,    â”‚â—€â”€â”€â”€â”€â”‚   Pipeline    â”‚â—€â”€â”€â”€â”€â”‚  (OpenClaw) â”‚
â”‚   Meet, etc) â”‚     â”‚  STT â†’ TTS   â”‚     â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The core abstraction is a **Channel Adapter** â€” a standard interface that any voice source implements. The pipeline handles:

1. **Ingest** â€” Receive audio from the channel
2. **STT** â€” Convert speech to text (Deepgram primary, Whisper fallback)
3. **Agent** â€” Route text to an OpenClaw agent session via WebSocket
4. **TTS** â€” Convert agent response to speech (ElevenLabs primary, Google TTS fallback)
5. **Output** â€” Stream audio back to the channel

See [ARCHITECTURE.md](ARCHITECTURE.md) for the full technical deep-dive.

## Project Structure

```
packages/
â”œâ”€â”€ core/       # Pipeline abstraction, channel interface, session management
â”œâ”€â”€ stt/        # Speech-to-text adapters (Deepgram, Whisper)
â”œâ”€â”€ tts/        # Text-to-speech adapters (ElevenLabs, Google TTS)
â””â”€â”€ channels/   # Channel adapters (Twilio, Meet, Slack, WebRTC)
```

## Quick Start

```bash
# Prerequisites: Node.js 20+, pnpm 9+
pnpm install
cp .env.example .env  # Configure API keys

# Start the voice server
pnpm dev
```

## Roadmap

See [ROADMAP.md](ROADMAP.md) for the phased development plan.

| Phase | Focus | Timeline |
|-------|-------|----------|
| 1 â€” Foundation | Core pipeline + Twilio | Weeks 1-3 |
| 2 â€” Channels | Meet, Slack, WebRTC | Weeks 4-6 |
| 3 â€” Intelligence | Context, barge-in, speaker ID | Weeks 7-9 |
| 4 â€” Platform | Dashboard, billing, API | Weeks 10-12 |

## Related Projects

- [voice-gateway](https://github.com/AlfeAI/voice-gateway) â€” Existing WebRTC + Google Meet voice infrastructure (being integrated into Alfe Voice)

## Tech Stack

- **Runtime:** TypeScript / Node.js
- **Monorepo:** pnpm workspaces
- **STT:** Deepgram (primary), Whisper (fallback)
- **TTS:** ElevenLabs (primary), Google TTS (fallback)
- **Phone:** Twilio
- **WebRTC:** Browser + Meet channels
- **Agent:** OpenClaw gateway WebSocket sessions

## License

MIT Â© [Alfe AI](https://alfe.ai)
